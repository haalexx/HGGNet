optimizer : {
  type: AdamW,
  kwargs: {
  lr : 0.0005,
  weight_decay : 0.0005
}}

scheduler: {
  type: LambdaLR,
  kwargs: {
  decay_step: 10,
  lr_decay: 0.9,
  lowest_decay: 0.002  # min lr = lowest_decay * lr
}}

bnmscheduler: {
  type: Lambda,
  kwargs: {
  decay_step: 10,
  bn_decay: 0.5,
  bn_momentum: 0.9,
  lowest_decay: 0.01
}}

model: {
  stage1_npoints: 600,
  stage2_npoints: 360,
  stage3_npoints: 64,
  folding_step: 4,
  global_channels: 640,
}
total_bs : 24
step_per_update : 1
max_epoch : 500

consider_metric: CDL1
